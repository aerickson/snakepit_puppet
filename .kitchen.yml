---
driver:
  name: docker
  use_sudo: false
  privileged: true
  run_command: /bin/systemd

provisioner:
  name: puppet_apply
  hiera_data_path: data
  hiera_data_remote_path: /tmp/kitchen/data
  hiera_deep_merge: true
  resolve_with_librarian_puppet: false
  manifests_path: manifests
  resolve_with_r10k: false
  puppet_debug: true
  puppet_verbose: true
  require_chef_for_busser: true
  require_puppet_omnibus: true
  require_puppet_repo: false
  #
  modules_path: modules:.modules
  #
  # name: puppet_bolt
  # bolt_version: 2.40.2  # last that works with ruby <2.5.3 (1804)
  # bolt_modulepath:  modules;.modules
  #


platforms:
  - name: ubuntu-18.04
    driver_config:
      image: ubuntu:bionic
      platform: ubuntu

busser:
  sudo: true

verifier:
  name: inspec

suites:
  - name: snakepit-head
    driver_config:
      hostname: mlchead.kitchen
    provisioner:
      custom_options: '-e "include roles::snakepit_head"'
  - name: snakepit-worker
    driver_config:
      hostname: mlc0.kitchen
    provisioner:
      custom_options: '-e "include roles::snakepit_worker"'

  # multi-node testing notes (couldn't get it working):
  #     (idea is that we could interact with head from worker)
  # - kitchen-docker's driver_config's link has a race, it needs the other container to be booted to be able to reference.
  # - kitchen-docker's 'use_internal_docker_network' didn't work, messes with hostnames
  # - for more, see
  #    http://www.hurryupandwait.io/blog/multi-node-testing-with-test-kitchen-and-docker

  # kitchen-puppet puppet_bolt options
  #     bolt_commands:
  #     - bolt --help
  #     - bolt --version
  #     - bolt apply manifests/snakepit_head.pp
  #     - bolt task show
  # how does it get hostname (to run locally)? or how do files get to the host (to run remotely)?
